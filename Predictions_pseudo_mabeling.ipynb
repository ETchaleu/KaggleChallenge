{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 0. PACKAGE IMPORTS\n",
        "# =========================================="
      ],
      "metadata": {
        "id": "KRgh_oX3qFpM"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install catboost\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from pandas import json_normalize\n",
        "from textblob import TextBlob # Nouvelle feature légère\n",
        "\n",
        "# Modèles Mac-Compatible\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, StackingClassifier, HistGradientBoostingClassifier, ExtraTreesClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Preprocessing\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import StandardScaler, OrdinalEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t2uhL6zaqLsK",
        "outputId": "d6b8f97f-f4e8-46f1-c76f-8414ee16bd38"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting catboost\n",
            "  Downloading catboost-1.2.8-cp312-cp312-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.12/dist-packages (from catboost) (0.21)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from catboost) (3.10.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.16.0 in /usr/local/lib/python3.12/dist-packages (from catboost) (2.0.2)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.12/dist-packages (from catboost) (2.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from catboost) (1.16.3)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.12/dist-packages (from catboost) (5.24.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from catboost) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (4.61.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (3.2.5)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.12/dist-packages (from plotly->catboost) (9.1.2)\n",
            "Downloading catboost-1.2.8-cp312-cp312-manylinux2014_x86_64.whl (99.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.2/99.2 MB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: catboost\n",
            "Successfully installed catboost-1.2.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 1. FEATURE ENGINEERING (ULTIME)\n",
        "# =========================================="
      ],
      "metadata": {
        "id": "LDtEgpiiqH8M"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_source(source_html):\n",
        "    if pd.isna(source_html): return \"Unknown\"\n",
        "    match = re.search(r'>(.*?)<', str(source_html))\n",
        "    return match.group(1) if match else \"Unknown\""
      ],
      "metadata": {
        "id": "3MSFzjo1qLVH"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_sentiment(text):\n",
        "    # Les bots sont souvent très neutres ou très positifs (spam commercial)\n",
        "    # Les humains sont plus nuancés (négatifs, sarcastiques)\n",
        "    try:\n",
        "        return TextBlob(text).sentiment.polarity\n",
        "    except:\n",
        "        return 0"
      ],
      "metadata": {
        "id": "lSOhZkKA1toy"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def feature_engineering(df):\n",
        "    print(\"... Feature Engineering ...\")\n",
        "\n",
        "    # Nettoyage\n",
        "    target_cols = ['user.listed_count', 'user.favourites_count', 'user.statuses_count',\n",
        "                   'quote_count', 'favorite_count', 'retweet_count', 'reply_count',\n",
        "                   'user.followers_count', 'user.friends_count']\n",
        "    for col in target_cols:\n",
        "        if col not in df.columns: df[col] = 0\n",
        "        df[col] = df[col].fillna(0)\n",
        "\n",
        "    # Ratios\n",
        "    df['log_followers'] = np.log1p(df['user.followers_count'])\n",
        "    df['log_friends'] = np.log1p(df['user.friends_count'])\n",
        "    df['ratio_log'] = df['log_followers'] - df['log_friends']\n",
        "    df['log_listed'] = np.log1p(df['user.listed_count'])\n",
        "    df['log_statuses'] = np.log1p(df['user.statuses_count'])\n",
        "    df['activity_ratio'] = (df['user.favourites_count'] + 1) / (df['user.statuses_count'] + 1)\n",
        "\n",
        "    # Temps\n",
        "    if 'created_at' in df.columns and 'user.created_at' in df.columns:\n",
        "        df['tweet_date'] = pd.to_datetime(df['created_at'], errors='coerce', utc=True).dt.tz_localize(None)\n",
        "        df['user_date'] = pd.to_datetime(df['user.created_at'], errors='coerce', utc=True).dt.tz_localize(None)\n",
        "        df['account_age_days'] = (df['tweet_date'] - df['user_date']).dt.days\n",
        "        df['account_age_days'] = df['account_age_days'].fillna(0)\n",
        "    else:\n",
        "        df['account_age_days'] = 0\n",
        "\n",
        "    df['growth_rate'] = df['user.followers_count'] / (df['account_age_days'] + 1)\n",
        "\n",
        "    # Text\n",
        "    def get_clean_text(row):\n",
        "        txt = str(row.get('text', ''))\n",
        "        if 'extended_tweet.full_text' in row and not pd.isna(row['extended_tweet.full_text']):\n",
        "            txt = str(row['extended_tweet.full_text'])\n",
        "        return txt\n",
        "    df['final_text'] = df.apply(get_clean_text, axis=1)\n",
        "\n",
        "    # --- NOUVEAU : Sentiment Analysis ---\n",
        "    df['sentiment'] = df['final_text'].apply(get_sentiment)\n",
        "\n",
        "    # Text Stats\n",
        "    df['lexical_diversity'] = df['final_text'].apply(lambda x: len(set(x.split())) / (len(x.split()) + 1))\n",
        "    df['caps_ratio'] = df['final_text'].apply(lambda x: sum(1 for c in x if c.isupper()) / (len(x)+1))\n",
        "    df['length_char'] = df['final_text'].apply(len)\n",
        "    df['num_hashtags'] = df['final_text'].apply(lambda x: x.count('#'))\n",
        "    df['num_mentions'] = df['final_text'].apply(lambda x: x.count('@'))\n",
        "\n",
        "    # Bio\n",
        "    if 'user.description' not in df.columns: df['user.description'] = \"\"\n",
        "    df['user_desc'] = df['user.description'].fillna(\"\")\n",
        "    df['desc_len'] = df['user_desc'].apply(len)\n",
        "\n",
        "    # Defaults\n",
        "    if 'user.default_profile_image' in df.columns:\n",
        "        df['is_default_image'] = df['user.default_profile_image'].fillna(False).astype(int)\n",
        "    else: df['is_default_image'] = 0\n",
        "\n",
        "    if 'user.default_profile' in df.columns:\n",
        "        df['is_default_profile'] = df['user.default_profile'].fillna(False).astype(int)\n",
        "    else: df['is_default_profile'] = 0\n",
        "\n",
        "    # Source\n",
        "    if 'source' not in df.columns: df['source'] = \"\"\n",
        "    df['source_clean'] = df['source'].apply(extract_source)\n",
        "    top_sources = ['Twitter for iPhone', 'Twitter for Android', 'Twitter Web App', 'TweetDeck', 'Hootsuite', 'Buffer']\n",
        "    df['source_category'] = df['source_clean'].apply(lambda x: x if x in top_sources else 'Other')\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "xj6fOCRtqSLZ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 2. PRÉPARATION DU PIPELINE\n",
        "# =========================================="
      ],
      "metadata": {
        "id": "N1XBsmUjqV-l"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_pipeline():\n",
        "    numeric_features = [\n",
        "        'user.listed_count', 'user.favourites_count', 'user.statuses_count',\n",
        "        'log_listed', 'log_statuses', 'ratio_log', 'activity_ratio', 'growth_rate',\n",
        "        'quote_count', 'favorite_count', 'retweet_count', 'reply_count',\n",
        "        'caps_ratio', 'length_char', 'lexical_diversity', 'num_hashtags', 'num_mentions', 'sentiment', # <-- Sentiment added\n",
        "        'desc_len', 'is_default_image', 'is_default_profile',\n",
        "        'account_age_days'\n",
        "    ]\n",
        "\n",
        "    categorical_features = ['source_category']\n",
        "    tweet_text_col = 'final_text'\n",
        "    desc_text_col = 'user_desc'\n",
        "\n",
        "    # Transformers\n",
        "    numeric_transformer = Pipeline(steps=[('imputer', SimpleImputer(strategy='median')), ('scaler', StandardScaler())])\n",
        "    categorical_transformer = Pipeline(steps=[('imputer', SimpleImputer(strategy='constant', fill_value='Other')),\n",
        "                                              ('encoder', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1))])\n",
        "\n",
        "    # TF-IDF reduit pour aller plus vite pendant le pseudo-labeling\n",
        "    tweet_text_transformer = Pipeline(steps=[\n",
        "        ('tfidf', TfidfVectorizer(max_features=3000, stop_words='english', ngram_range=(1,2))),\n",
        "        ('svd', TruncatedSVD(n_components=30, random_state=42))\n",
        "    ])\n",
        "\n",
        "    desc_text_transformer = Pipeline(steps=[\n",
        "        ('tfidf', TfidfVectorizer(max_features=500, stop_words='english')),\n",
        "        ('svd', TruncatedSVD(n_components=5, random_state=42))\n",
        "    ])\n",
        "\n",
        "    preprocessor = ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('num', numeric_transformer, numeric_features),\n",
        "            ('cat', categorical_transformer, categorical_features),\n",
        "            ('tweet_txt', tweet_text_transformer, tweet_text_col),\n",
        "            ('desc_txt', desc_text_transformer, desc_text_col)\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    # Stack Mac-Safe\n",
        "    estimators = [\n",
        "        ('hgb', HistGradientBoostingClassifier(max_iter=200, learning_rate=0.05, max_depth=10, random_state=42)),\n",
        "        ('cat', CatBoostClassifier(iterations=200, depth=8, learning_rate=0.05, verbose=0, random_seed=42)),\n",
        "        ('rf', RandomForestClassifier(n_estimators=150, max_depth=15, random_state=42, n_jobs=-1)),\n",
        "        ('et', ExtraTreesClassifier(n_estimators=150, max_depth=15, random_state=42, n_jobs=-1))\n",
        "    ]\n",
        "\n",
        "    stacking_clf = StackingClassifier(estimators=estimators, final_estimator=LogisticRegression(), cv=5, n_jobs=-1)\n",
        "\n",
        "    return Pipeline(steps=[('preprocessor', preprocessor), ('classifier', stacking_clf)])"
      ],
      "metadata": {
        "id": "WbLrJr8X13Tq"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 3. EXÉCUTION : PSEUDO-LABELING\n",
        "# =========================================="
      ],
      "metadata": {
        "id": "MhwXPYoq16-5"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_pseudo_labeling():\n",
        "    print(\"--- 1. Chargement & Engineering ---\")\n",
        "    train_df = pd.read_json('train.jsonl', lines=True)\n",
        "    test_df = pd.read_json('kaggle_test.jsonl', lines=True)\n",
        "\n",
        "    train_df = json_normalize(train_df.to_dict(orient='records'))\n",
        "    test_df = json_normalize(test_df.to_dict(orient='records'))\n",
        "\n",
        "    train_df = feature_engineering(train_df)\n",
        "    test_df = feature_engineering(test_df)\n",
        "\n",
        "    X_train = train_df\n",
        "    y_train = train_df['label']\n",
        "    X_test = test_df\n",
        "\n",
        "    # --- ROUND 1 : Entraînement Initial ---\n",
        "    print(\"\\n--- 2. Entraînement Initial (Round 1) ---\")\n",
        "    model = get_pipeline()\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Prédiction des probabilités sur le test\n",
        "    print(\"Génération des Pseudo-Labels...\")\n",
        "    probs = model.predict_proba(X_test) # [[prob_0, prob_1], ...]\n",
        "\n",
        "    # --- PSEUDO LABELING LOGIC ---\n",
        "    # On prend les prédictions où le modèle est sûr à > 98%\n",
        "    # On crée un DataFrame temporaire\n",
        "    pseudo_labels = []\n",
        "\n",
        "    # Probabilité d'être un BOT (Classe 1) > 0.98\n",
        "    high_conf_bots = np.where(probs[:, 1] > 0.98)[0]\n",
        "    # Probabilité d'être HUMAIN (Classe 0) > 0.98 (donc prob bot < 0.02)\n",
        "    high_conf_humans = np.where(probs[:, 1] < 0.02)[0]\n",
        "\n",
        "    print(f\"Trouvé {len(high_conf_bots)} Bots 'sûrs' et {len(high_conf_humans)} Humains 'sûrs' dans le Test Set.\")\n",
        "\n",
        "    # Création du dataset augmenté\n",
        "    # On prend les lignes du test set correspondantes\n",
        "    test_bots = X_test.iloc[high_conf_bots].copy()\n",
        "    test_bots['label'] = 1 # On force le label\n",
        "\n",
        "    test_humans = X_test.iloc[high_conf_humans].copy()\n",
        "    test_humans['label'] = 0 # On force le label\n",
        "\n",
        "    # Fusion avec le train set original\n",
        "    X_train_augmented = pd.concat([X_train, test_bots, test_humans], axis=0)\n",
        "    # Important: mettre à jour y_train aussi (il est dans le dataframe X_train_augmented maintenant)\n",
        "    y_train_augmented = X_train_augmented['label']\n",
        "\n",
        "    print(f\"Taille Training Original : {len(X_train)}\")\n",
        "    print(f\"Taille Training Augmenté : {len(X_train_augmented)} (+{len(X_train_augmented)-len(X_train)})\")\n",
        "\n",
        "    # --- ROUND 2 : Entraînement Final sur données augmentées ---\n",
        "    print(\"\\n--- 3. Entraînement Final (Round 2 - Pseudo Labeling) ---\")\n",
        "    # On recrée un modèle neuf\n",
        "    final_model = get_pipeline()\n",
        "    final_model.fit(X_train_augmented, y_train_augmented)\n",
        "\n",
        "    print(\"--- 4. Prédiction Finale ---\")\n",
        "    final_predictions = final_model.predict(X_test)\n",
        "\n",
        "    submission = pd.DataFrame({'ID': test_df['challenge_id'], 'Prediction': final_predictions})\n",
        "    submission.to_csv('Prediction_pseudo_labeling.csv', index=False)\n",
        "\n",
        "    print(\"Fichier 'Prediction_pseudo_labeling.csv' prêt.\")"
      ],
      "metadata": {
        "id": "eFflIosAIyvB"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    run_pseudo_labeling()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fdYgXNfYqaUZ",
        "outputId": "123f31da-7ed4-45cf-b52d-52ad0af1656a"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- 1. Chargement & Engineering ---\n",
            "... Feature Engineering ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-482181727.py:23: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  df['user_date'] = pd.to_datetime(df['user.created_at'], errors='coerce', utc=True).dt.tz_localize(None)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "... Feature Engineering ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-482181727.py:23: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  df['user_date'] = pd.to_datetime(df['user.created_at'], errors='coerce', utc=True).dt.tz_localize(None)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- 2. Entraînement Initial (Round 1) ---\n",
            "Génération des Pseudo-Labels...\n",
            "Trouvé 0 Bots 'sûrs' et 0 Humains 'sûrs' dans le Test Set.\n",
            "Taille Training Original : 154914\n",
            "Taille Training Augmenté : 154914 (+0)\n",
            "\n",
            "--- 3. Entraînement Final (Round 2 - Pseudo Labeling) ---\n",
            "--- 4. Prédiction Finale ---\n",
            "Fichier 'Prediction_pseudo_labeling.csv' prêt.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}