{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 0. PACKAGE IMPORTS\n",
    "# =========================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "from pandas import json_normalize\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder, FunctionTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 1. FEATURE ENGINEERING\n",
    "# =========================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_regex(pattern, text):\n",
    "    if not isinstance(text, str): return 0\n",
    "    return len(re.findall(pattern, text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_source(source_html):\n",
    "    if pd.isna(source_html): return \"Unknown\"\n",
    "    match = re.search(r'>(.*?)<', str(source_html))\n",
    "    return match.group(1) if match else \"Unknown\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 1.2 TEXT ANALYSIS\n",
    "# =========================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweetDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset PyTorch pour des tweets stockés dans un DataFrame pandas.\n",
    "\n",
    "    - df : DataFrame avec au moins une colonne texte et éventuellement 'label'\n",
    "    - text_col : nom de la colonne contenant le tweet (ici 'full_text')\n",
    "    - label_col : nom de la colonne des labels (None pour le test Kaggle)\n",
    "    \"\"\"\n",
    "    def __init__(self, df, tokenizer, text_col=\"full_text\", label_col=\"label\", max_length=128):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.text_col = text_col\n",
    "        self.label_col = label_col\n",
    "        self.max_length = max_length\n",
    "        self.has_labels = label_col is not None and label_col in df.columns\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Récupérer le texte brut dans le DataFrame\n",
    "        text = str(self.df.iloc[idx][self.text_col])\n",
    "\n",
    "        # Tokenization pour CamemBERT\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        item = {\n",
    "            \"input_ids\": encoding[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].squeeze(0),\n",
    "        }\n",
    "\n",
    "        if self.has_labels:\n",
    "            label = int(self.df.iloc[idx][self.label_col])\n",
    "            item[\"labels\"] = torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "        return item\n",
    "\n",
    "\n",
    "\n",
    "class ClassificationHead(nn.Module):\n",
    "    \"\"\"\n",
    "    Tête de classification modulaire.\n",
    "    Tu pourras facilement changer hidden_dim, activation, etc.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int,\n",
    "        num_labels: int,\n",
    "        hidden_dim: int = None,\n",
    "        dropout: float = 0.1,\n",
    "        activation: str = \"gelu\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        if hidden_dim is None:\n",
    "            hidden_dim = input_dim\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear1 = nn.Linear(input_dim, hidden_dim)\n",
    "\n",
    "        if activation == \"gelu\":\n",
    "            self.activation = nn.GELU()\n",
    "        elif activation == \"relu\":\n",
    "            self.activation = nn.ReLU()\n",
    "        else:\n",
    "            raise ValueError(f\"Activation inconnue: {activation}\")\n",
    "\n",
    "        self.linear2 = nn.Linear(hidden_dim, num_labels)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class CamembertForCustomClassification(nn.Module):\n",
    "    \"\"\"\n",
    "    Wrapper CamemBERT + tête de classification.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str,\n",
    "        num_labels: int = 2,\n",
    "        dropout: float = 0.1,\n",
    "        pooling: str = \"cls\",      # 'cls' ou 'mean'\n",
    "        hidden_dim: int = None,\n",
    "        activation: str = \"gelu\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.backbone = AutoModel.from_pretrained(model_name)\n",
    "        hidden_size = self.backbone.config.hidden_size\n",
    "\n",
    "        self.pooling = pooling\n",
    "        self.num_labels = num_labels\n",
    "\n",
    "        self.classifier = ClassificationHead(\n",
    "            input_dim=hidden_size,\n",
    "            num_labels=num_labels,\n",
    "            hidden_dim=hidden_dim,\n",
    "            dropout=dropout,\n",
    "            activation=activation,\n",
    "        )\n",
    "\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    def _pool(self, last_hidden_state: torch.Tensor, attention_mask: torch.Tensor):\n",
    "        if self.pooling == \"cls\":\n",
    "            return last_hidden_state[:, 0, :]\n",
    "        elif self.pooling == \"mean\":\n",
    "            mask = attention_mask.unsqueeze(-1).expand(last_hidden_state.size())\n",
    "            masked = last_hidden_state * mask\n",
    "            summed = masked.sum(dim=1)\n",
    "            counts = mask.sum(dim=1).clamp(min=1)\n",
    "            return summed / counts\n",
    "        else:\n",
    "            raise ValueError(f\"Pooling inconnue: {self.pooling}\")\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.Tensor,\n",
    "        attention_mask: torch.Tensor = None,\n",
    "        labels: torch.Tensor = None,\n",
    "    ):\n",
    "        outputs = self.backbone(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "        )\n",
    "        last_hidden_state = outputs.last_hidden_state\n",
    "        pooled = self._pool(last_hidden_state, attention_mask)\n",
    "\n",
    "        logits = self.classifier(pooled)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss = self.loss_fn(logits, labels)\n",
    "\n",
    "        return {\"logits\": logits, \"loss\": loss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_name = \"Yanzhu/bertweetfr-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "model = CamembertForCustomClassification(\n",
    "    model_name=model_name,   # ou bertweetfr, etc.\n",
    "    num_labels=2,\n",
    ")\n",
    "model.load_state_dict(torch.load(\"Model1.pth\", map_location=device))\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def predict_from_text(model, data_loader, device):\n",
    "    \"\"\"\n",
    "    Effectue l'inférence sur data_loader et écrit un CSV format Kaggle :\n",
    "    \n",
    "    csv_path : chemin du CSV de sortie\n",
    "    \"\"\"\n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    all_preds = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(data_loader, leave=False):\n",
    "\n",
    "            # Récupération input\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask\n",
    "            )\n",
    "\n",
    "            logits = outputs[\"logits\"]\n",
    "            probas = torch.softmax(logits, dim=1)[:, 1].cpu().tolist()\n",
    "\n",
    "            all_preds.extend(probas)\n",
    "    \n",
    "    # Creation DataFrame Kaggle\n",
    "    df = pd.DataFrame({\n",
    "        \"Prediction\": all_preds\n",
    "    })\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering(df):\n",
    "    print(\"--- Génération des features ---\")\n",
    "    \n",
    "    # --- A. Nettoyage de base ---\n",
    "    # Remplir les valeurs nulles numériques\n",
    "    num_cols = ['user.followers_count', 'user.friends_count', 'user.listed_count', \n",
    "                'user.favourites_count', 'user.statuses_count']\n",
    "    for col in num_cols:\n",
    "        if col not in df.columns: df[col] = 0\n",
    "        df[col] = df[col].fillna(0)\n",
    "\n",
    "    # --- B. Le \"Log Ratio\" ---\n",
    "    df['log_followers'] = np.log1p(df['user.followers_count'])\n",
    "    df['log_friends'] = np.log1p(df['user.friends_count'])\n",
    "    df['ratio_log'] = df['log_followers'] - df['log_friends']\n",
    "    df['user_listed_ratio'] = df['user.listed_count'] / (df['user.followers_count'] + 1)\n",
    "    \n",
    "    # --- C. Analyse de la Biographie (user.description) ---\n",
    "    # C'est la nouveauté majeure.\n",
    "    df['user_desc'] = df['user.description'].fillna(\"\")\n",
    "    \n",
    "    # 1. Longueur de la bio (les pros soignent leur bio)\n",
    "    df['desc_len'] = df['user_desc'].apply(len)\n",
    "    \n",
    "    # 2. Signaux de professionnalisme dans la bio\n",
    "    # Détection d'email (contact pro) ou de liens\n",
    "    df['desc_has_email'] = df['user_desc'].apply(lambda x: 1 if re.search(r'[\\w\\.-]+@[\\w\\.-]+', x) else 0)\n",
    "    df['desc_has_link'] = df['user_desc'].apply(lambda x: 1 if \"http\" in x else 0)\n",
    "    \n",
    "    # 3. Mots clés \"Pro\" (comptage manuel simple)\n",
    "    pro_keywords = ['journaliste', 'journalist', 'official', 'officiel', 'media', 'news', 'presse', 'auteur', 'author']\n",
    "    df['desc_is_pro'] = df['user_desc'].apply(lambda x: 1 if any(word in x.lower() for word in pro_keywords) else 0)\n",
    "\n",
    "    # --- D. Analyse du Tweet (Contenu et Forme) ---\n",
    "    def get_text(row):\n",
    "        text = str(row.get('text', ''))\n",
    "        if 'extended_tweet.full_text' in row and not pd.isna(row['extended_tweet.full_text']):\n",
    "            text = str(row['extended_tweet.full_text'])\n",
    "        return text\n",
    "    \n",
    "    df['final_text'] = df.apply(get_text, axis=1)\n",
    "    \n",
    "    # Features \"Psycholinguistiques\"\n",
    "    # Usage excessif de majuscules (souvent signe d'amateurisme ou de colère)\n",
    "    df['caps_ratio'] = df['final_text'].apply(lambda x: sum(1 for c in x if c.isupper()) / (len(x)+1))\n",
    "    # Longueur moyenne des mots (vocabulaire riche vs simple)\n",
    "    df['avg_word_len'] = df['final_text'].apply(lambda x: np.mean([len(w) for w in x.split()]) if len(x.split()) > 0 else 0)\n",
    "    # Ponctuation\n",
    "    df['exclamation_count'] = df['final_text'].apply(lambda x: x.count('!'))\n",
    "\n",
    "    # --- E. Source ---\n",
    "    df['source_clean'] = df['source'].apply(extract_source)\n",
    "    top_sources = ['Twitter for iPhone', 'Twitter for Android', 'Twitter Web App', 'TweetDeck', 'iPad', 'Hootsuite', 'Buffer']\n",
    "    df['source_category'] = df['source_clean'].apply(lambda x: x if x in top_sources else 'Other')\n",
    "    \n",
    "    # --- F. Meta Tweet ---\n",
    "    if 'entities.hashtags' in df.columns:\n",
    "        df['num_hashtags'] = df['entities.hashtags'].apply(lambda x: len(x) if isinstance(x, list) else 0)\n",
    "    else: df['num_hashtags'] = 0\n",
    "\n",
    "    if 'entities.urls' in df.columns:\n",
    "        df['num_urls'] = df['entities.urls'].apply(lambda x: len(x) if isinstance(x, list) else 0)\n",
    "    else: df['num_urls'] = 0\n",
    "\n",
    "    # --- G. Text analysis features ---\n",
    "\n",
    "    print(\"--- Extraction du texte complet et analyse textuelle ---\")\n",
    "\n",
    "    # Tweets can be truncated, storing the full version in 'extended_tweet.full_text'.\n",
    "    def extract_full_text(tweet):\n",
    "        text = tweet['text']\n",
    "        if not pd.isna(tweet['extended_tweet.full_text']):\n",
    "            text = tweet['extended_tweet.full_text']\n",
    "        return text\n",
    "\n",
    "    # Apply this function to every row (axis=1)\n",
    "    df['full_text'] = df.apply(lambda tweet: extract_full_text(tweet), axis=1)\n",
    "    \n",
    "    kaggle_dataset = TweetDataset(\n",
    "        df=df,\n",
    "        tokenizer=tokenizer,\n",
    "        text_col=\"full_text\",\n",
    "        label_col=None,      # pas de labels pour Kaggle\n",
    "        max_length=128,\n",
    "    )\n",
    "\n",
    "    kaggle_loader = DataLoader(kaggle_dataset, batch_size=32, shuffle=False)\n",
    "    text_analysis_df = predict_from_text(model, kaggle_loader, device)\n",
    "    df['text_analysis_pred'] = text_analysis_df['Prediction']\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 2. PIPELINE DE TRAITEMENT\n",
    "# =========================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run():\n",
    "    print(\"Chargement des données...\")\n",
    "    # Assurez-vous que les fichiers sont au bon endroit\n",
    "    train_df = pd.read_json('train.jsonl', lines=True)\n",
    "    test_df = pd.read_json('kaggle_test.jsonl', lines=True)\n",
    "\n",
    "    print(\"Feature Engineering...\")\n",
    "    train_df = json_normalize(train_df.to_dict(orient='records'))\n",
    "    test_df = json_normalize(test_df.to_dict(orient='records'))\n",
    "\n",
    "    train_df = feature_engineering(train_df)\n",
    "    test_df = feature_engineering(test_df)\n",
    "\n",
    "    # --- Sélecteur de colonnes ---\n",
    "    \n",
    "    numeric_features = [\n",
    "        'user.followers_count', 'user.friends_count', 'user.listed_count', \n",
    "        'user.favourites_count', 'user.statuses_count',\n",
    "        'ratio_log', 'user_listed_ratio',\n",
    "        'desc_len', 'desc_has_email', 'desc_has_link', 'desc_is_pro', # Bio stats\n",
    "        'name_digits_count', 'name_len', # User name stats\n",
    "        'caps_ratio', 'avg_word_len', 'exclamation_count', # Style stats\n",
    "        'num_hashtags', 'num_urls', 'retweet_count', 'favorite_count', 'text_analysis_pred'\n",
    "    ]\n",
    "    \n",
    "    categorical_features = ['source_category']\n",
    "    \n",
    "    tweet_text_col = 'final_text'\n",
    "    desc_text_col = 'user_desc' # On traite aussi le TEXTE de la bio via TF-IDF\n",
    "\n",
    "    # --- Transformers ---\n",
    "\n",
    "    # 1. Numérique\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "\n",
    "    # 2. Catégoriel\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='constant', fill_value='Other')),\n",
    "        ('encoder', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1))\n",
    "    ])\n",
    "\n",
    "    # 3. Texte du Tweet (SVD 50 composants)\n",
    "    tweet_text_transformer = Pipeline(steps=[\n",
    "        ('tfidf', TfidfVectorizer(max_features=5000, stop_words='english', ngram_range=(1,2))),\n",
    "        ('svd', TruncatedSVD(n_components=50, random_state=42))\n",
    "    ])\n",
    "\n",
    "    # 4. Texte de la Bio (SVD 10 composants - c'est plus court)\n",
    "    # Cela permet de capter des thèmes (\"Politique\", \"Sport\") dans la bio\n",
    "    desc_text_transformer = Pipeline(steps=[\n",
    "        ('tfidf', TfidfVectorizer(max_features=1000, stop_words='english')),\n",
    "        ('svd', TruncatedSVD(n_components=10, random_state=42))\n",
    "    ])\n",
    "\n",
    "    # Assemblage global\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numeric_transformer, numeric_features),\n",
    "            ('cat', categorical_transformer, categorical_features),\n",
    "            ('tweet_txt', tweet_text_transformer, tweet_text_col),\n",
    "            ('desc_txt', desc_text_transformer, desc_text_col)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # --- Modèle ---\n",
    "    # On augmente légèrement la complexité (profondeur) car on a plus de données\n",
    "    model = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', HistGradientBoostingClassifier(\n",
    "            max_iter=300,        # Plus d'itérations\n",
    "            learning_rate=0.05,  # Apprentissage plus fin (plus lent mais plus précis)\n",
    "            max_depth=12,        # Arbres un peu plus profonds\n",
    "            max_leaf_nodes=40,\n",
    "            l2_regularization=1.0, # Évite le sur-apprentissage\n",
    "            random_state=42,\n",
    "            scoring='accuracy'\n",
    "        ))\n",
    "    ])\n",
    "\n",
    "    # --- Exécution ---\n",
    "    X = train_df\n",
    "    y = train_df['label']\n",
    "\n",
    "    print(\"-\" * 30)\n",
    "    print(\"Validation Croisée (5-Fold)...\")\n",
    "    kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    scores = cross_val_score(model, X, y, cv=kfold, scoring='accuracy', n_jobs=-1)\n",
    "    \n",
    "    print(f\"Scores CV : {scores}\")\n",
    "    print(f\"Moyenne CV : {np.mean(scores)*100:.2f}% (+/- {np.std(scores)*100:.2f}%)\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "    print(\"Entraînement Final...\")\n",
    "    model.fit(X, y)\n",
    "\n",
    "    print(\"Prédiction...\")\n",
    "    predictions = model.predict(test_df)\n",
    "\n",
    "    submission = pd.DataFrame({'ID': test_df['challenge_id'], 'Prediction': predictions})\n",
    "    submission.to_csv('submission_expert.csv', index=False)\n",
    "    print(\"Fichier 'submission_expert.csv' prêt.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chargement des données...\n",
      "Feature Engineering...\n",
      "--- Génération des features ---\n",
      "ATTENTION : Colonne 'screen_name' introuvable. Remplie avec des vides.\n",
      "--- Génération des features ---\n",
      "ATTENTION : Colonne 'screen_name' introuvable. Remplie avec des vides.\n",
      "------------------------------\n",
      "Validation Croisée (5-Fold)...\n",
      "Scores CV : [0.84097731 0.82574315 0.83875028 0.8331343  0.82983668]\n",
      "Moyenne CV : 83.37% (+/- 0.56%)\n",
      "------------------------------\n",
      "Entraînement Final...\n",
      "Prédiction...\n",
      "Fichier 'submission_expert.csv' prêt.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
