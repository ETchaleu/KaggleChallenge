{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 0. PACKAGE IMPORTS\n",
        "# =========================================="
      ],
      "metadata": {
        "id": "KRgh_oX3qFpM"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import joblib\n",
        "from pandas import json_normalize\n",
        "\n",
        "# Les 3 Rois du Boosting\n",
        "from xgboost import XGBClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Preprocessing\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import StandardScaler, OrdinalEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD"
      ],
      "metadata": {
        "id": "t2uhL6zaqLsK"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 1. FEATURE ENGINEERING (ULTIME)\n",
        "# =========================================="
      ],
      "metadata": {
        "id": "LDtEgpiiqH8M"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_source(source_html):\n",
        "    if pd.isna(source_html): return \"Unknown\"\n",
        "    match = re.search(r'>(.*?)<', str(source_html))\n",
        "    return match.group(1) if match else \"Unknown\""
      ],
      "metadata": {
        "id": "3MSFzjo1qLVH"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def feature_engineering(df):\n",
        "    print(\"... Génération des Features ...\")\n",
        "\n",
        "    # 1. Nettoyage des colonnes manquantes\n",
        "    target_cols = ['user.listed_count', 'user.favourites_count', 'user.statuses_count',\n",
        "                   'quote_count', 'favorite_count', 'retweet_count', 'reply_count',\n",
        "                   'user.followers_count', 'user.friends_count']\n",
        "    for col in target_cols:\n",
        "        if col not in df.columns: df[col] = 0\n",
        "        df[col] = df[col].fillna(0)\n",
        "\n",
        "    # 2. Les Ratios Mathématiques (Crucial pour les arbres)\n",
        "    df['log_followers'] = np.log1p(df['user.followers_count'])\n",
        "    df['log_friends'] = np.log1p(df['user.friends_count'])\n",
        "    df['ratio_log'] = df['log_followers'] - df['log_friends']\n",
        "    df['log_listed'] = np.log1p(df['user.listed_count'])\n",
        "    df['log_statuses'] = np.log1p(df['user.statuses_count'])\n",
        "\n",
        "    # Ratio d'activité (Likes donnés / Tweets postés)\n",
        "    # +1 pour éviter la division par zéro\n",
        "    df['activity_ratio'] = (df['user.favourites_count'] + 1) / (df['user.statuses_count'] + 1)\n",
        "\n",
        "    # 3. Gestion du Temps (Âge du compte)\n",
        "    if 'created_at' in df.columns and 'user.created_at' in df.columns:\n",
        "        # Conversion robuste\n",
        "        df['tweet_date'] = pd.to_datetime(df['created_at'], errors='coerce', utc=True).dt.tz_localize(None)\n",
        "        df['user_date'] = pd.to_datetime(df['user.created_at'], errors='coerce', utc=True).dt.tz_localize(None)\n",
        "        df['account_age_days'] = (df['tweet_date'] - df['user_date']).dt.days\n",
        "        df['account_age_days'] = df['account_age_days'].fillna(0)\n",
        "    else:\n",
        "        df['account_age_days'] = 0\n",
        "\n",
        "    # Growth Rate (Followers par jour d'existence) - Très fort indicateur\n",
        "    df['growth_rate'] = df['user.followers_count'] / (df['account_age_days'] + 1)\n",
        "\n",
        "    # 4. Text Mining (Basique mais efficace)\n",
        "    def get_clean_text(row):\n",
        "        txt = str(row.get('text', ''))\n",
        "        if 'extended_tweet.full_text' in row and not pd.isna(row['extended_tweet.full_text']):\n",
        "            txt = str(row['extended_tweet.full_text'])\n",
        "        return txt\n",
        "    df['final_text'] = df.apply(get_clean_text, axis=1)\n",
        "\n",
        "    def calc_diversity(text):\n",
        "        words = text.split()\n",
        "        if len(words) == 0: return 0\n",
        "        return len(set(words)) / len(words)\n",
        "\n",
        "    df['lexical_diversity'] = df['final_text'].apply(calc_diversity)\n",
        "    df['caps_ratio'] = df['final_text'].apply(lambda x: sum(1 for c in x if c.isupper()) / (len(x)+1))\n",
        "    df['length_char'] = df['final_text'].apply(len)\n",
        "    df['num_hashtags'] = df['final_text'].apply(lambda x: x.count('#'))\n",
        "    df['num_mentions'] = df['final_text'].apply(lambda x: x.count('@'))\n",
        "\n",
        "    # 5. Profil (Bio & Défauts)\n",
        "    if 'user.description' not in df.columns: df['user.description'] = \"\"\n",
        "    df['user_desc'] = df['user.description'].fillna(\"\")\n",
        "    df['desc_len'] = df['user_desc'].apply(len)\n",
        "\n",
        "    # Mots clés Pro\n",
        "    pro_keywords = ['official', 'officiel', 'media', 'news', 'presse', 'journaliste']\n",
        "    df['desc_is_pro'] = df['user_desc'].apply(lambda x: 1 if any(w in x.lower() for w in pro_keywords) else 0)\n",
        "\n",
        "    # --- NOUVEAUTÉS : Indicateurs \"Default\" ---\n",
        "    if 'user.default_profile_image' in df.columns:\n",
        "        df['is_default_image'] = df['user.default_profile_image'].fillna(False).astype(int)\n",
        "    else:\n",
        "        df['is_default_image'] = 0 # Par défaut on assume que non\n",
        "\n",
        "    if 'user.default_profile' in df.columns:\n",
        "        df['is_default_profile'] = df['user.default_profile'].fillna(False).astype(int)\n",
        "    else:\n",
        "        df['is_default_profile'] = 0\n",
        "\n",
        "    # Source\n",
        "    if 'source' not in df.columns: df['source'] = \"\"\n",
        "    df['source_clean'] = df['source'].apply(extract_source)\n",
        "    # On garde les Top sources, les autres deviennent \"Other\"\n",
        "    top_sources = ['Twitter for iPhone', 'Twitter for Android', 'Twitter Web App', 'TweetDeck', 'Hootsuite', 'Buffer']\n",
        "    df['source_category'] = df['source_clean'].apply(lambda x: x if x in top_sources else 'Other')\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "xj6fOCRtqSLZ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 2. PRÉPARATION DU PIPELINE\n",
        "# =========================================="
      ],
      "metadata": {
        "id": "N1XBsmUjqV-l"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_stacking():\n",
        "    print(\"--- 1. Chargement des données ---\")\n",
        "    train_df = pd.read_json('train.jsonl', lines=True)\n",
        "    test_df = pd.read_json('kaggle_test.jsonl', lines=True)\n",
        "\n",
        "    train_df = json_normalize(train_df.to_dict(orient='records'))\n",
        "    test_df = json_normalize(test_df.to_dict(orient='records'))\n",
        "\n",
        "    train_df = feature_engineering(train_df)\n",
        "    test_df = feature_engineering(test_df)\n",
        "\n",
        "    # LISTE DES FEATURES (La plus complète à ce jour)\n",
        "    numeric_features = [\n",
        "        # Stats User\n",
        "        'user.listed_count', 'user.favourites_count', 'user.statuses_count',\n",
        "        'log_listed', 'log_statuses', 'ratio_log', 'activity_ratio', 'growth_rate',\n",
        "        # Stats Tweet\n",
        "        'quote_count', 'favorite_count', 'retweet_count', 'reply_count',\n",
        "        # Text Analysis\n",
        "        'caps_ratio', 'length_char', 'lexical_diversity', 'num_hashtags', 'num_mentions',\n",
        "        # Bio & Profile\n",
        "        'desc_len', 'desc_is_pro', 'is_default_image', 'is_default_profile',\n",
        "        'account_age_days'\n",
        "    ]\n",
        "\n",
        "    categorical_features = ['source_category']\n",
        "    tweet_text_col = 'final_text'\n",
        "    desc_text_col = 'user_desc'\n",
        "\n",
        "    # --- Transformers ---\n",
        "    numeric_transformer = Pipeline(steps=[\n",
        "        ('imputer', SimpleImputer(strategy='median')),\n",
        "        ('scaler', StandardScaler())\n",
        "    ])\n",
        "\n",
        "    categorical_transformer = Pipeline(steps=[\n",
        "        ('imputer', SimpleImputer(strategy='constant', fill_value='Other')),\n",
        "        ('encoder', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1))\n",
        "    ])\n",
        "\n",
        "    # TF-IDF + SVD (Beaucoup plus robuste que BERT pour les arbres)\n",
        "    tweet_text_transformer = Pipeline(steps=[\n",
        "        ('tfidf', TfidfVectorizer(max_features=5000, stop_words='english', ngram_range=(1,2))),\n",
        "        ('svd', TruncatedSVD(n_components=50, random_state=42))\n",
        "    ])\n",
        "\n",
        "    desc_text_transformer = Pipeline(steps=[\n",
        "        ('tfidf', TfidfVectorizer(max_features=1000, stop_words='english')),\n",
        "        ('svd', TruncatedSVD(n_components=10, random_state=42))\n",
        "    ])\n",
        "\n",
        "    preprocessor = ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('num', numeric_transformer, numeric_features),\n",
        "            ('cat', categorical_transformer, categorical_features),\n",
        "            ('tweet_txt', tweet_text_transformer, tweet_text_col),\n",
        "            ('desc_txt', desc_text_transformer, desc_text_col)\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    # ==========================================\n",
        "    # 3. LE STACKING (LA SAINTE TRINITÉ)\n",
        "    # ==========================================\n",
        "    print(\"--- 2. Configuration du Stacking ---\")\n",
        "\n",
        "    # Niveau 1 : Les Modèles de Base (Base Learners)\n",
        "    estimators = [\n",
        "        # XGBoost : Le standard de l'industrie\n",
        "        ('xgb', XGBClassifier(n_estimators=300, max_depth=8, learning_rate=0.05,\n",
        "                              eval_metric='logloss', random_state=42, n_jobs=-1)),\n",
        "\n",
        "        # LightGBM : Très rapide et différent de XGB\n",
        "        ('lgbm', LGBMClassifier(n_estimators=300, num_leaves=31, learning_rate=0.05,\n",
        "                                random_state=42, n_jobs=-1, verbose=-1)),\n",
        "\n",
        "        # CatBoost : Excellent pour gérer les relations complexes\n",
        "        ('cat', CatBoostClassifier(iterations=300, depth=8, learning_rate=0.05,\n",
        "                                   verbose=0, random_seed=42)),\n",
        "\n",
        "        # RandomForest : Pour apporter de la variance (Bagging vs Boosting)\n",
        "        ('rf', RandomForestClassifier(n_estimators=200, max_depth=15, random_state=42, n_jobs=-1))\n",
        "    ]\n",
        "\n",
        "    # Niveau 2 : Le Meta-Learner\n",
        "    # Il apprend à combiner les avis des 4 modèles précédents\n",
        "    stacking_clf = StackingClassifier(\n",
        "        estimators=estimators,\n",
        "        final_estimator=LogisticRegression(),\n",
        "        cv=5, # Cross-Validation interne pour éviter le sur-apprentissage\n",
        "        n_jobs=-1\n",
        "    )\n",
        "\n",
        "    model = Pipeline(steps=[\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('classifier', stacking_clf)\n",
        "    ])\n",
        "\n",
        "    # ==========================================\n",
        "    # 4. ENTRAÎNEMENT & PRÉDICTION\n",
        "    # ==========================================\n",
        "    X = train_df\n",
        "    y = train_df['label']\n",
        "\n",
        "    print(\"--- 3. Entraînement du Stack (Cela peut prendre 2-3 min) ---\")\n",
        "    # Le Stacking entraîne les modèles 5 fois chacun (CV), soyez patient\n",
        "    model.fit(X, y)\n",
        "\n",
        "    print(\"--- 4. Prédiction ---\")\n",
        "    predictions = model.predict(test_df)\n",
        "\n",
        "    submission = pd.DataFrame({'ID': test_df['challenge_id'], 'Prediction': predictions})\n",
        "    submission.to_csv('Prediction_stacking.csv', index=False)\n",
        "\n",
        "    print(\"Fichier 'Prediction_stacking.csv' généré.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "eFflIosAIyvB"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    run_stacking()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fdYgXNfYqaUZ",
        "outputId": "3d4d0dd3-8c47-4618-cbea-adefd91431d1"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- 1. Chargement des données ---\n",
            "... Génération des Features ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1510795777.py:27: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  df['user_date'] = pd.to_datetime(df['user.created_at'], errors='coerce', utc=True).dt.tz_localize(None)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "... Génération des Features ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1510795777.py:27: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  df['user_date'] = pd.to_datetime(df['user.created_at'], errors='coerce', utc=True).dt.tz_localize(None)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- 2. Configuration du Stacking ---\n",
            "--- 3. Entraînement du Stack (Cela peut prendre 2-3 min) ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/joblib/externals/loky/process_executor.py:782: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- 4. Prédiction ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fichier 'Prediction_stacking.csv' généré.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}