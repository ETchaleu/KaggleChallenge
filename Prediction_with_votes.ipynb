{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 0. PACKAGE IMPORTS\n",
    "# =========================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "from pandas import json_normalize\n",
    "from datetime import datetime\n",
    "import joblib\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier, RandomForestClassifier, ExtraTreesClassifier, VotingClassifier\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 1. FEATURES ENGINEERING (VERSION 5 - TIME & LEXICAL)\n",
    "# =========================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_source(source_html):\n",
    "    if pd.isna(source_html): return \"Unknown\"\n",
    "    match = re.search(r'>(.*?)<', str(source_html))\n",
    "    return match.group(1) if match else \"Unknown\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering(df):\n",
    "    print(\"--- Génération des features ---\")\n",
    "    \n",
    "    # --- A. Nettoyage Numérique ---\n",
    "    target_cols = [\n",
    "        'user.listed_count', 'user.favourites_count', 'user.statuses_count',\n",
    "        'quote_count', 'favorite_count', 'retweet_count', 'reply_count',\n",
    "        'user.followers_count', 'user.friends_count'\n",
    "    ]\n",
    "    for col in target_cols:\n",
    "        if col not in df.columns: df[col] = 0\n",
    "        df[col] = df[col].fillna(0)\n",
    "\n",
    "    # --- B. Ratios Mathématiques ---\n",
    "    df['log_followers'] = np.log1p(df['user.followers_count'])\n",
    "    df['log_friends'] = np.log1p(df['user.friends_count'])\n",
    "    df['ratio_log'] = df['log_followers'] - df['log_friends']\n",
    "    df['log_listed'] = np.log1p(df['user.listed_count'])\n",
    "    df['log_statuses'] = np.log1p(df['user.statuses_count'])\n",
    "\n",
    "    # --- C. Gestion du TEMPS (CORRECTION CRASH) ---\n",
    "    # Twitter format: \"Fri Nov 06 10:58:12 +0000 2015\"\n",
    "    \n",
    "    # 1. On convertit en forçant UTC=True pour harmoniser\n",
    "    # 2. On utilise .dt.tz_localize(None) pour RETIRER la timezone et rendre les dates \"naïves\" (comparables)\n",
    "    if 'created_at' in df.columns and 'user.created_at' in df.columns:\n",
    "        df['tweet_date'] = pd.to_datetime(df['created_at'], errors='coerce', utc=True).dt.tz_localize(None)\n",
    "        df['user_date'] = pd.to_datetime(df['user.created_at'], errors='coerce', utc=True).dt.tz_localize(None)\n",
    "        \n",
    "        # Calcul de l'âge du compte en jours\n",
    "        df['account_age_days'] = (df['tweet_date'] - df['user_date']).dt.days\n",
    "        df['account_age_days'] = df['account_age_days'].fillna(0)\n",
    "    else:\n",
    "        df['account_age_days'] = 0\n",
    "\n",
    "    # --- D. Analyse du TEXTE Avancée ---\n",
    "    def get_clean_text(row):\n",
    "        txt = str(row.get('text', ''))\n",
    "        if 'extended_tweet.full_text' in row and not pd.isna(row['extended_tweet.full_text']):\n",
    "            txt = str(row['extended_tweet.full_text'])\n",
    "        return txt\n",
    "\n",
    "    df['final_text'] = df.apply(get_clean_text, axis=1)\n",
    "    \n",
    "    # Diversité Lexicale\n",
    "    def calc_diversity(text):\n",
    "        words = text.split()\n",
    "        if len(words) == 0: return 0\n",
    "        return len(set(words)) / len(words)\n",
    "\n",
    "    df['lexical_diversity'] = df['final_text'].apply(calc_diversity)\n",
    "    df['caps_ratio'] = df['final_text'].apply(lambda x: sum(1 for c in x if c.isupper()) / (len(x)+1))\n",
    "    df['exclamation_count'] = df['final_text'].apply(lambda x: x.count('!'))\n",
    "    df['length_char'] = df['final_text'].apply(len)\n",
    "\n",
    "    # --- E. Bio & Source ---\n",
    "    if 'user.description' not in df.columns: df['user.description'] = \"\"\n",
    "    df['user_desc'] = df['user.description'].fillna(\"\")\n",
    "    \n",
    "    df['desc_len'] = df['user_desc'].apply(len)\n",
    "    df['desc_has_email'] = df['user_desc'].str.contains(r'[\\w\\.-]+@[\\w\\.-]+', regex=True).fillna(0).astype(int)\n",
    "    df['desc_has_http'] = df['user_desc'].str.contains(r'http', regex=True).fillna(0).astype(int)\n",
    "    \n",
    "    pro_keywords = ['official', 'officiel', 'media', 'news', 'presse', 'journaliste']\n",
    "    df['desc_is_pro'] = df['user_desc'].apply(lambda x: 1 if any(w in x.lower() for w in pro_keywords) else 0)\n",
    "\n",
    "    if 'source' not in df.columns: df['source'] = \"\"\n",
    "    df['source_clean'] = df['source'].apply(extract_source)\n",
    "    top_sources = ['Twitter for iPhone', 'Twitter for Android', 'Twitter Web App', 'TweetDeck']\n",
    "    df['source_category'] = df['source_clean'].apply(lambda x: x if x in top_sources else 'Other')\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 2. PIPELINE DE VOTING\n",
    "# =========================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run():\n",
    "    print(\"Chargement des données...\")\n",
    "    train_df = pd.read_json('train.jsonl', lines=True)\n",
    "    test_df = pd.read_json('kaggle_test.jsonl', lines=True)\n",
    "\n",
    "    print(\"Feature Engineering...\")\n",
    "    train_df = json_normalize(train_df.to_dict(orient='records'))\n",
    "    test_df = json_normalize(test_df.to_dict(orient='records'))\n",
    "\n",
    "    train_df = feature_engineering(train_df)\n",
    "    test_df = feature_engineering(test_df)\n",
    "\n",
    "    # Liste Mise à jour\n",
    "    numeric_features = [\n",
    "        'user.listed_count', 'user.favourites_count', 'user.statuses_count',\n",
    "        'log_listed', 'log_statuses', 'ratio_log', \n",
    "        'quote_count', 'favorite_count', 'retweet_count', 'reply_count',\n",
    "        'caps_ratio', 'exclamation_count', 'length_char', 'lexical_diversity', # <-- Diversity\n",
    "        'desc_len', 'desc_has_email', 'desc_has_http', 'desc_is_pro',\n",
    "        'account_age_days' # <-- Time feature\n",
    "    ]\n",
    "    \n",
    "    categorical_features = ['source_category']\n",
    "    tweet_text_col = 'final_text'\n",
    "    desc_text_col = 'user_desc'\n",
    "\n",
    "    # --- Transformers ---\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='constant', fill_value='Other')),\n",
    "        ('encoder', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1))\n",
    "    ])\n",
    "\n",
    "    tweet_text_transformer = Pipeline(steps=[\n",
    "        ('tfidf', TfidfVectorizer(max_features=5000, stop_words='english', ngram_range=(1,2))), # Ngrams ajoutés\n",
    "        ('svd', TruncatedSVD(n_components=50, random_state=42))\n",
    "    ])\n",
    "\n",
    "    desc_text_transformer = Pipeline(steps=[\n",
    "        ('tfidf', TfidfVectorizer(max_features=1000, stop_words='english')),\n",
    "        ('svd', TruncatedSVD(n_components=10, random_state=42))\n",
    "    ])\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numeric_transformer, numeric_features),\n",
    "            ('cat', categorical_transformer, categorical_features),\n",
    "            ('tweet_txt', tweet_text_transformer, tweet_text_col),\n",
    "            ('desc_txt', desc_text_transformer, desc_text_col)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # --- MODÈLE ENSEMBLE (VOTING) ---\n",
    "    # On combine 3 modèles complémentaires\n",
    "    \n",
    "    clf1 = HistGradientBoostingClassifier(max_iter=300, learning_rate=0.05, max_depth=10, random_state=42)\n",
    "    clf2 = RandomForestClassifier(n_estimators=300, max_depth=15, random_state=42, n_jobs=-1)\n",
    "    clf3 = ExtraTreesClassifier(n_estimators=300, max_depth=15, random_state=42, n_jobs=-1)\n",
    "\n",
    "    voting_model = VotingClassifier(\n",
    "        estimators=[\n",
    "            ('hgb', clf1), \n",
    "            ('rf', clf2),\n",
    "            ('et', clf3)\n",
    "        ],\n",
    "        voting='soft' # 'soft' utilise les probabilités, souvent meilleur que 'hard'\n",
    "    )\n",
    "\n",
    "    model = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', voting_model)\n",
    "    ])\n",
    "\n",
    "    # --- Exécution ---\n",
    "    X = train_df\n",
    "    y = train_df['label']\n",
    "\n",
    "    print(\"-\" * 30)\n",
    "    print(\"Validation Croisée (5-Fold) sur l'Ensemble...\")\n",
    "    kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    scores = cross_val_score(model, X, y, cv=kfold, scoring='accuracy', n_jobs=-1)\n",
    "    \n",
    "    print(f\"Scores CV : {scores}\")\n",
    "    print(f\"Moyenne CV : {np.mean(scores)*100:.2f}% (+/- {np.std(scores)*100:.2f}%)\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "    print(\"Entraînement Final & Prédiction...\")\n",
    "    model.fit(X, y)\n",
    "\n",
    "    # --- SAUVEGARDE DU MODÈLE ---\n",
    "    print(\"Sauvegarde du modèle en cours...\")\n",
    "    joblib.dump(model, 'model_detector_bots_v5.pkl')\n",
    "    print(\"Modèle sauvegardé sous 'model_detector_bots_v5.pkl'\")\n",
    "    # -----------------------------\n",
    "    \n",
    "    predictions = model.predict(test_df)\n",
    "\n",
    "    submission = pd.DataFrame({'ID': test_df['challenge_id'], 'Prediction': predictions})\n",
    "    submission.to_csv('submission_v5_ensemble.csv', index=False)\n",
    "    print(\"Fichier 'submission_v5_ensemble.csv' prêt. Bonne chance !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chargement des données...\n",
      "Feature Engineering V5...\n",
      "--- Génération des features (Time & Lexical - CORRIGÉ) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/10/sxk44tnd01761c9swjc098jr0000gn/T/ipykernel_26353/835469491.py:28: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df['user_date'] = pd.to_datetime(df['user.created_at'], errors='coerce', utc=True).dt.tz_localize(None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Génération des features (Time & Lexical - CORRIGÉ) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/10/sxk44tnd01761c9swjc098jr0000gn/T/ipykernel_26353/835469491.py:28: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df['user_date'] = pd.to_datetime(df['user.created_at'], errors='coerce', utc=True).dt.tz_localize(None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Validation Croisée (5-Fold) sur l'Ensemble...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python(29475) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(29476) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(29478) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(29481) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(29483) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(29486) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(29487) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(29489) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(29491) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(29493) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(29495) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(29497) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores CV : [0.84923991 0.84659329 0.84930446 0.84730336 0.84336066]\n",
      "Moyenne CV : 84.72% (+/- 0.22%)\n",
      "------------------------------\n",
      "Entraînement Final & Prédiction...\n",
      "Sauvegarde du modèle en cours...\n",
      "Modèle sauvegardé sous 'model_detector_bots_v5.pkl'\n",
      "Fichier 'submission_v5_ensemble.csv' prêt. Bonne chance !\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
