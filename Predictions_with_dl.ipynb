{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Bm5tLTHxMf4I"
      },
      "outputs": [],
      "source": [
        "# ==========================================\n",
        "# 0. PACKAGE IMPORTS\n",
        "# =========================================="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "TTS6yGRtMf4J"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import json\n",
        "\n",
        "# Deep Learning Imports\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense, Embedding, Bidirectional, LSTM, GlobalMaxPool1D, Concatenate, Dropout, BatchNormalization\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Sklearn pour la préparation\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from pandas import json_normalize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "FsPW4Bl4Mf4J"
      },
      "outputs": [],
      "source": [
        "# ==========================================\n",
        "# 1. FEATURES ENGINEERING\n",
        "# =========================================="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "-rqJ9fSjMf4J"
      },
      "outputs": [],
      "source": [
        "def extract_source(source_html):\n",
        "    if pd.isna(source_html): return \"Unknown\"\n",
        "    match = re.search(r'>(.*?)<', str(source_html))\n",
        "    return match.group(1) if match else \"Unknown\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "A7EoewJgMf4J"
      },
      "outputs": [],
      "source": [
        "def feature_engineering(df):\n",
        "    target_cols = ['user.listed_count', 'user.favourites_count', 'user.statuses_count',\n",
        "                   'quote_count', 'favorite_count', 'retweet_count', 'reply_count',\n",
        "                   'user.followers_count', 'user.friends_count']\n",
        "    for col in target_cols:\n",
        "        if col not in df.columns: df[col] = 0\n",
        "        df[col] = df[col].fillna(0)\n",
        "\n",
        "    df['log_followers'] = np.log1p(df['user.followers_count'])\n",
        "    df['log_friends'] = np.log1p(df['user.friends_count'])\n",
        "    df['ratio_log'] = df['log_followers'] - df['log_friends']\n",
        "    df['log_listed'] = np.log1p(df['user.listed_count'])\n",
        "    df['log_statuses'] = np.log1p(df['user.statuses_count'])\n",
        "\n",
        "    # Time\n",
        "    if 'created_at' in df.columns and 'user.created_at' in df.columns:\n",
        "        df['tweet_date'] = pd.to_datetime(df['created_at'], errors='coerce', utc=True).dt.tz_localize(None)\n",
        "        df['user_date'] = pd.to_datetime(df['user.created_at'], errors='coerce', utc=True).dt.tz_localize(None)\n",
        "        df['account_age_days'] = (df['tweet_date'] - df['user_date']).dt.days\n",
        "        df['account_age_days'] = df['account_age_days'].fillna(0)\n",
        "    else:\n",
        "        df['account_age_days'] = 0\n",
        "\n",
        "    # Text cleaning\n",
        "    def get_clean_text(row):\n",
        "        txt = str(row.get('text', ''))\n",
        "        if 'extended_tweet.full_text' in row and not pd.isna(row['extended_tweet.full_text']):\n",
        "            txt = str(row['extended_tweet.full_text'])\n",
        "        return txt\n",
        "    df['final_text'] = df.apply(get_clean_text, axis=1)\n",
        "\n",
        "    # Text Stats\n",
        "    def calc_diversity(text):\n",
        "        words = text.split()\n",
        "        if len(words) == 0: return 0\n",
        "        return len(set(words)) / len(words)\n",
        "    df['lexical_diversity'] = df['final_text'].apply(calc_diversity)\n",
        "    df['caps_ratio'] = df['final_text'].apply(lambda x: sum(1 for c in x if c.isupper()) / (len(x)+1))\n",
        "    df['exclamation_count'] = df['final_text'].apply(lambda x: x.count('!'))\n",
        "    df['length_char'] = df['final_text'].apply(len)\n",
        "\n",
        "    # Bio\n",
        "    if 'user.description' not in df.columns: df['user.description'] = \"\"\n",
        "    df['user_desc'] = df['user.description'].fillna(\"\")\n",
        "    df['desc_len'] = df['user_desc'].apply(len)\n",
        "    df['desc_has_email'] = df['user_desc'].str.contains(r'[\\w\\.-]+@[\\w\\.-]+', regex=True).fillna(0).astype(int)\n",
        "    df['desc_has_http'] = df['user_desc'].str.contains(r'http', regex=True).fillna(0).astype(int)\n",
        "    pro_keywords = ['official', 'officiel', 'media', 'news', 'presse', 'journaliste']\n",
        "    df['desc_is_pro'] = df['user_desc'].apply(lambda x: 1 if any(w in x.lower() for w in pro_keywords) else 0)\n",
        "\n",
        "    # Source encoding (simple categorical for NN)\n",
        "    if 'source' not in df.columns: df['source'] = \"\"\n",
        "    df['source_clean'] = df['source'].apply(extract_source)\n",
        "    top_sources = ['Twitter for iPhone', 'Twitter for Android', 'Twitter Web App', 'TweetDeck']\n",
        "    df['source_is_top'] = df['source_clean'].apply(lambda x: 1 if x in top_sources else 0)\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "j9hPZbpiMf4K"
      },
      "outputs": [],
      "source": [
        "# ==========================================\n",
        "# 2. PIPELINE\n",
        "# =========================================="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "zCkc0xuQMf4K"
      },
      "outputs": [],
      "source": [
        "def run_dl():\n",
        "    print(\"--- 1. Chargement & Feature Engineering ---\")\n",
        "    train_df = pd.read_json('train.jsonl', lines=True)\n",
        "    test_df = pd.read_json('kaggle_test.jsonl', lines=True)\n",
        "\n",
        "    train_df = json_normalize(train_df.to_dict(orient='records'))\n",
        "    test_df = json_normalize(test_df.to_dict(orient='records'))\n",
        "\n",
        "    train_df = feature_engineering(train_df)\n",
        "    test_df = feature_engineering(test_df)\n",
        "\n",
        "    # --- SÉLECTION DES FEATURES NUMÉRIQUES ---\n",
        "    numeric_features = [\n",
        "        'user.listed_count', 'user.favourites_count', 'user.statuses_count',\n",
        "        'log_listed', 'log_statuses', 'ratio_log',\n",
        "        'quote_count', 'favorite_count', 'retweet_count', 'reply_count',\n",
        "        'caps_ratio', 'exclamation_count', 'length_char', 'lexical_diversity',\n",
        "        'desc_len', 'desc_has_email', 'desc_has_http', 'desc_is_pro',\n",
        "        'account_age_days', 'source_is_top'\n",
        "    ]\n",
        "\n",
        "    # Normalisation (CRITIQUE pour les réseaux de neurones)\n",
        "    scaler = StandardScaler()\n",
        "    X_train_meta = scaler.fit_transform(train_df[numeric_features])\n",
        "    X_test_meta = scaler.transform(test_df[numeric_features])\n",
        "\n",
        "    y_train = train_df['label'].values\n",
        "\n",
        "    # --- PRÉPARATION DU TEXTE (TOKENIZATION) ---\n",
        "    print(\"--- 2. Tokenization du Texte (NLP) ---\")\n",
        "    MAX_WORDS = 20000     # Taille du vocabulaire\n",
        "    MAX_LEN = 100         # Longueur max d'un tweet analysé\n",
        "\n",
        "    tokenizer = Tokenizer(num_words=MAX_WORDS, oov_token=\"<OOV>\")\n",
        "    tokenizer.fit_on_texts(train_df['final_text'])\n",
        "\n",
        "    X_train_text = tokenizer.texts_to_sequences(train_df['final_text'])\n",
        "    X_test_text = tokenizer.texts_to_sequences(test_df['final_text'])\n",
        "\n",
        "    # Padding (pour avoir des vecteurs de même longueur)\n",
        "    X_train_text = pad_sequences(X_train_text, maxlen=MAX_LEN, padding='post', truncating='post')\n",
        "    X_test_text = pad_sequences(X_test_text, maxlen=MAX_LEN, padding='post', truncating='post')\n",
        "\n",
        "    print(f\"Vocabulaire : {len(tokenizer.word_index)} mots uniques\")\n",
        "    print(f\"Shape Meta : {X_train_meta.shape} | Shape Text : {X_train_text.shape}\")\n",
        "\n",
        "    # ==========================================\n",
        "    # 3. ARCHITECTURE DU MODÈLE HYBRIDE\n",
        "    # ==========================================\n",
        "\n",
        "    print(\"--- 3. Construction du Cerveau Hybride ---\")\n",
        "\n",
        "    # --- BRANCHE A : TEXTE (LSTM) ---\n",
        "    input_text = Input(shape=(MAX_LEN,), name=\"input_text\")\n",
        "    # Embedding : Transforme les mots en vecteurs denses\n",
        "    x_text = Embedding(input_dim=MAX_WORDS, output_dim=128, input_length=MAX_LEN)(input_text)\n",
        "    # SpatialDropout pour éviter le sur-apprentissage sur certains mots\n",
        "    x_text = tf.keras.layers.SpatialDropout1D(0.2)(x_text)\n",
        "    # Bidirectional LSTM : Lit dans les deux sens\n",
        "    x_text = Bidirectional(LSTM(64, return_sequences=True))(x_text)\n",
        "    x_text = GlobalMaxPool1D()(x_text) # Garde l'info la plus forte\n",
        "\n",
        "    # --- BRANCHE B : METADATA (DENSE) ---\n",
        "    input_meta = Input(shape=(X_train_meta.shape[1],), name=\"input_meta\")\n",
        "    x_meta = Dense(64, activation='relu')(input_meta)\n",
        "    x_meta = BatchNormalization()(x_meta)\n",
        "    x_meta = Dropout(0.3)(x_meta)\n",
        "    x_meta = Dense(32, activation='relu')(x_meta)\n",
        "\n",
        "    # --- FUSION ---\n",
        "    x = Concatenate()([x_text, x_meta])\n",
        "    x = Dense(64, activation='relu')(x)\n",
        "    x = Dropout(0.3)(x)\n",
        "    x = Dense(32, activation='relu')(x)\n",
        "    output = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "    model = Model(inputs=[input_text, input_meta], outputs=output)\n",
        "\n",
        "    # Compilation\n",
        "    model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])\n",
        "    model.summary()\n",
        "\n",
        "    # ==========================================\n",
        "    # 4. ENTRAÎNEMENT\n",
        "    # ==========================================\n",
        "\n",
        "    print(\"--- 4. Entraînement ---\")\n",
        "\n",
        "    # Callbacks pour sauver le meilleur modèle et arrêter si ça stagne\n",
        "    checkpoint = ModelCheckpoint(\"best_model_dl.h5\", monitor='val_accuracy', save_best_only=True, mode='max', verbose=1)\n",
        "    early_stop = EarlyStopping(monitor='val_accuracy', patience=5, restore_best_weights=True, verbose=1)\n",
        "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, verbose=1)\n",
        "\n",
        "    # On utilise 20% du train set pour valider pendant l'entrainement\n",
        "    history = model.fit(\n",
        "        [X_train_text, X_train_meta], y_train,\n",
        "        batch_size=32,\n",
        "        epochs=20, # On met 20, mais early_stop arrêtera avant si nécessaire\n",
        "        validation_split=0.2,\n",
        "        callbacks=[checkpoint, early_stop, reduce_lr]\n",
        "    )\n",
        "\n",
        "    # ==========================================\n",
        "    # 5. PRÉDICTION\n",
        "    # ==========================================\n",
        "\n",
        "    print(\"--- 5. Prédiction Finale ---\")\n",
        "    # On recharge le meilleur checkpoint pour être sûr\n",
        "    model.load_weights(\"best_model_dl.h5\")\n",
        "\n",
        "    preds = model.predict([X_test_text, X_test_meta], batch_size=32)\n",
        "    # Sigmoid sort une proba entre 0 et 1. On coupe à 0.5\n",
        "    preds_binary = (preds > 0.5).astype(int).reshape(-1)\n",
        "\n",
        "    submission = pd.DataFrame({'ID': test_df['challenge_id'], 'Prediction': preds_binary})\n",
        "    submission.to_csv('Prediction_dl.csv', index=False)\n",
        "    print(\"Fichier 'Prediction_dl.csv' généré avec succès !\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "C5zQ-ej2Mf4L",
        "outputId": "de6642e0-f056-4df5-d07b-843aa1d4583c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- 1. Chargement & Feature Engineering ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-759328151.py:18: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  df['user_date'] = pd.to_datetime(df['user.created_at'], errors='coerce', utc=True).dt.tz_localize(None)\n",
            "/tmp/ipython-input-759328151.py:18: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  df['user_date'] = pd.to_datetime(df['user.created_at'], errors='coerce', utc=True).dt.tz_localize(None)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- 2. Tokenization du Texte (NLP) ---\n",
            "Vocabulaire : 221685 mots uniques\n",
            "Shape Meta : (154914, 20) | Shape Text : (154914, 100)\n",
            "--- 3. Construction du Cerveau Hybride ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_text          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ input_meta          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ embedding           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │  \u001b[38;5;34m2,560,000\u001b[0m │ input_text[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
              "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │      \u001b[38;5;34m1,344\u001b[0m │ input_meta[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ spatial_dropout1d   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ embedding[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "│ (\u001b[38;5;33mSpatialDropout1D\u001b[0m)  │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ batch_normalization │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │        \u001b[38;5;34m256\u001b[0m │ dense[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ bidirectional       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │     \u001b[38;5;34m98,816\u001b[0m │ spatial_dropout1… │\n",
              "│ (\u001b[38;5;33mBidirectional\u001b[0m)     │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ global_max_pooling… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ bidirectional[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mGlobalMaxPooling1…\u001b[0m │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │      \u001b[38;5;34m2,080\u001b[0m │ dropout[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ concatenate         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m160\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ global_max_pooli… │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ dense_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │     \u001b[38;5;34m10,304\u001b[0m │ concatenate[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ dense_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │      \u001b[38;5;34m2,080\u001b[0m │ dropout_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │         \u001b[38;5;34m33\u001b[0m │ dense_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_text          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ input_meta          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ embedding           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,560,000</span> │ input_text[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,344</span> │ input_meta[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ spatial_dropout1d   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SpatialDropout1D</span>)  │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ batch_normalization │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │ dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ bidirectional       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">98,816</span> │ spatial_dropout1… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)     │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ global_max_pooling… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ bidirectional[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling1…</span> │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │ dropout[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ concatenate         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ global_max_pooli… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ dense_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │     <span style=\"color: #00af00; text-decoration-color: #00af00\">10,304</span> │ concatenate[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │ dropout_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> │ dense_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,674,913\u001b[0m (10.20 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,674,913</span> (10.20 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,674,785\u001b[0m (10.20 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,674,785</span> (10.20 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m128\u001b[0m (512.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> (512.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- 4. Entraînement ---\n",
            "Epoch 1/20\n",
            "\u001b[1m3873/3873\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - accuracy: 0.7960 - loss: 0.4561\n",
            "Epoch 1: val_accuracy improved from -inf to 0.82910, saving model to best_model_dl.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m3873/3873\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m587s\u001b[0m 150ms/step - accuracy: 0.7960 - loss: 0.4561 - val_accuracy: 0.8291 - val_loss: 0.3910 - learning_rate: 0.0010\n",
            "Epoch 2/20\n",
            "\u001b[1m3873/3873\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - accuracy: 0.8335 - loss: 0.3815\n",
            "Epoch 2: val_accuracy did not improve from 0.82910\n",
            "\u001b[1m3873/3873\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m595s\u001b[0m 143ms/step - accuracy: 0.8335 - loss: 0.3815 - val_accuracy: 0.8277 - val_loss: 0.3936 - learning_rate: 0.0010\n",
            "Epoch 3/20\n",
            "\u001b[1m3873/3873\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - accuracy: 0.8561 - loss: 0.3365\n",
            "Epoch 3: val_accuracy did not improve from 0.82910\n",
            "\n",
            "Epoch 3: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "\u001b[1m3873/3873\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m555s\u001b[0m 143ms/step - accuracy: 0.8561 - loss: 0.3365 - val_accuracy: 0.8206 - val_loss: 0.4171 - learning_rate: 0.0010\n",
            "Epoch 4/20\n",
            "\u001b[1m3873/3873\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - accuracy: 0.8851 - loss: 0.2731\n",
            "Epoch 4: val_accuracy did not improve from 0.82910\n",
            "\u001b[1m3873/3873\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m567s\u001b[0m 146ms/step - accuracy: 0.8851 - loss: 0.2731 - val_accuracy: 0.8054 - val_loss: 0.4703 - learning_rate: 5.0000e-04\n",
            "Epoch 5/20\n",
            "\u001b[1m3873/3873\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - accuracy: 0.9075 - loss: 0.2223\n",
            "Epoch 5: val_accuracy did not improve from 0.82910\n",
            "\n",
            "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "\u001b[1m3873/3873\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m556s\u001b[0m 144ms/step - accuracy: 0.9075 - loss: 0.2223 - val_accuracy: 0.7852 - val_loss: 0.5658 - learning_rate: 5.0000e-04\n",
            "Epoch 6/20\n",
            "\u001b[1m3873/3873\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - accuracy: 0.9285 - loss: 0.1742\n",
            "Epoch 6: val_accuracy did not improve from 0.82910\n",
            "\u001b[1m3873/3873\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m563s\u001b[0m 144ms/step - accuracy: 0.9285 - loss: 0.1742 - val_accuracy: 0.7885 - val_loss: 0.6969 - learning_rate: 2.5000e-04\n",
            "Epoch 6: early stopping\n",
            "Restoring model weights from the end of the best epoch: 1.\n",
            "--- 5. Prédiction Finale ---\n",
            "\u001b[1m3231/3231\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 32ms/step\n",
            "Fichier 'Prediction_dl.csv' généré avec succès !\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    run_dl()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}