{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 0. PACKAGE IMPORTS\n",
    "# =========================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "from pandas import json_normalize\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder, FunctionTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 1. FEATURE ENGINEERING\n",
    "# =========================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_regex(pattern, text):\n",
    "    if not isinstance(text, str): return 0\n",
    "    return len(re.findall(pattern, text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_source(source_html):\n",
    "    if pd.isna(source_html): return \"Unknown\"\n",
    "    match = re.search(r'>(.*?)<', str(source_html))\n",
    "    return match.group(1) if match else \"Unknown\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering(df):\n",
    "    print(\"--- Génération des features ---\")\n",
    "    \n",
    "    # --- A. Nettoyage de base ---\n",
    "    # Remplir les valeurs nulles numériques\n",
    "    num_cols = ['user.followers_count', 'user.friends_count', 'user.listed_count', \n",
    "                'user.favourites_count', 'user.statuses_count']\n",
    "    for col in num_cols:\n",
    "        if col not in df.columns: df[col] = 0\n",
    "        df[col] = df[col].fillna(0)\n",
    "\n",
    "    # --- B. Le \"Log Ratio\" ---\n",
    "    df['log_followers'] = np.log1p(df['user.followers_count'])\n",
    "    df['log_friends'] = np.log1p(df['user.friends_count'])\n",
    "    df['ratio_log'] = df['log_followers'] - df['log_friends']\n",
    "    df['user_listed_ratio'] = df['user.listed_count'] / (df['user.followers_count'] + 1)\n",
    "    \n",
    "    # --- C. Analyse de la Biographie (user.description) ---\n",
    "    # C'est la nouveauté majeure.\n",
    "    df['user_desc'] = df['user.description'].fillna(\"\")\n",
    "    \n",
    "    # 1. Longueur de la bio (les pros soignent leur bio)\n",
    "    df['desc_len'] = df['user_desc'].apply(len)\n",
    "    \n",
    "    # 2. Signaux de professionnalisme dans la bio\n",
    "    # Détection d'email (contact pro) ou de liens\n",
    "    df['desc_has_email'] = df['user_desc'].apply(lambda x: 1 if re.search(r'[\\w\\.-]+@[\\w\\.-]+', x) else 0)\n",
    "    df['desc_has_link'] = df['user_desc'].apply(lambda x: 1 if \"http\" in x else 0)\n",
    "    \n",
    "    # 3. Mots clés \"Pro\" (comptage manuel simple)\n",
    "    pro_keywords = ['journaliste', 'journalist', 'official', 'officiel', 'media', 'news', 'presse', 'auteur', 'author']\n",
    "    df['desc_is_pro'] = df['user_desc'].apply(lambda x: 1 if any(word in x.lower() for word in pro_keywords) else 0)\n",
    "\n",
    "    # --- D. Analyse du Pseudo (Screen Name) ---\n",
    "    \n",
    "    # ÉTAPE DE SÉCURITÉ : Vérification de l'existence de la colonne\n",
    "    # On cherche 'user.screen_name', sinon 'screen_name', sinon on crée du vide.\n",
    "    if 'user.screen_name' in df.columns:\n",
    "        source_col = 'user.screen_name'\n",
    "    elif 'screen_name' in df.columns:\n",
    "        source_col = 'screen_name'\n",
    "    else:\n",
    "        # Si la colonne est totalement absente, on crée une colonne vide temporaire\n",
    "        df['user.screen_name'] = \"\"\n",
    "        source_col = 'user.screen_name'\n",
    "        print(\"ATTENTION : Colonne 'screen_name' introuvable. Remplie avec des vides.\")\n",
    "\n",
    "    df['screen_name'] = df[source_col].fillna(\"\")\n",
    "    \n",
    "    # Les bots/observateurs ont souvent beaucoup de chiffres à la fin du pseudo\n",
    "    df['name_digits_count'] = df['screen_name'].apply(lambda x: sum(c.isdigit() for c in x))\n",
    "    df['name_len'] = df['screen_name'].apply(len)\n",
    "\n",
    "    # --- E. Analyse du Tweet (Contenu et Forme) ---\n",
    "    def get_text(row):\n",
    "        text = str(row.get('text', ''))\n",
    "        if 'extended_tweet.full_text' in row and not pd.isna(row['extended_tweet.full_text']):\n",
    "            text = str(row['extended_tweet.full_text'])\n",
    "        return text\n",
    "    \n",
    "    df['final_text'] = df.apply(get_text, axis=1)\n",
    "    \n",
    "    # Features \"Psycholinguistiques\"\n",
    "    # Usage excessif de majuscules (souvent signe d'amateurisme ou de colère)\n",
    "    df['caps_ratio'] = df['final_text'].apply(lambda x: sum(1 for c in x if c.isupper()) / (len(x)+1))\n",
    "    # Longueur moyenne des mots (vocabulaire riche vs simple)\n",
    "    df['avg_word_len'] = df['final_text'].apply(lambda x: np.mean([len(w) for w in x.split()]) if len(x.split()) > 0 else 0)\n",
    "    # Ponctuation\n",
    "    df['exclamation_count'] = df['final_text'].apply(lambda x: x.count('!'))\n",
    "\n",
    "    # --- F. Source ---\n",
    "    df['source_clean'] = df['source'].apply(extract_source)\n",
    "    top_sources = ['Twitter for iPhone', 'Twitter for Android', 'Twitter Web App', 'TweetDeck', 'iPad', 'Hootsuite', 'Buffer']\n",
    "    df['source_category'] = df['source_clean'].apply(lambda x: x if x in top_sources else 'Other')\n",
    "    \n",
    "    # --- G. Meta Tweet ---\n",
    "    if 'entities.hashtags' in df.columns:\n",
    "        df['num_hashtags'] = df['entities.hashtags'].apply(lambda x: len(x) if isinstance(x, list) else 0)\n",
    "    else: df['num_hashtags'] = 0\n",
    "\n",
    "    if 'entities.urls' in df.columns:\n",
    "        df['num_urls'] = df['entities.urls'].apply(lambda x: len(x) if isinstance(x, list) else 0)\n",
    "    else: df['num_urls'] = 0\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 2. PIPELINE DE TRAITEMENT\n",
    "# =========================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run():\n",
    "    print(\"Chargement des données...\")\n",
    "    # Assurez-vous que les fichiers sont au bon endroit\n",
    "    train_df = pd.read_json('train.jsonl', lines=True)\n",
    "    test_df = pd.read_json('kaggle_test.jsonl', lines=True)\n",
    "\n",
    "    print(\"Feature Engineering...\")\n",
    "    train_df = json_normalize(train_df.to_dict(orient='records'))\n",
    "    test_df = json_normalize(test_df.to_dict(orient='records'))\n",
    "\n",
    "    train_df = feature_engineering(train_df)\n",
    "    test_df = feature_engineering(test_df)\n",
    "\n",
    "    # --- Sélecteur de colonnes ---\n",
    "    \n",
    "    numeric_features = [\n",
    "        'user.followers_count', 'user.friends_count', 'user.listed_count', \n",
    "        'user.favourites_count', 'user.statuses_count',\n",
    "        'ratio_log', 'user_listed_ratio',\n",
    "        'desc_len', 'desc_has_email', 'desc_has_link', 'desc_is_pro', # Bio stats\n",
    "        'name_digits_count', 'name_len', # User name stats\n",
    "        'caps_ratio', 'avg_word_len', 'exclamation_count', # Style stats\n",
    "        'num_hashtags', 'num_urls', 'retweet_count', 'favorite_count'\n",
    "    ]\n",
    "    \n",
    "    categorical_features = ['source_category']\n",
    "    \n",
    "    tweet_text_col = 'final_text'\n",
    "    desc_text_col = 'user_desc' # On traite aussi le TEXTE de la bio via TF-IDF\n",
    "\n",
    "    # --- Transformers ---\n",
    "\n",
    "    # 1. Numérique\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "\n",
    "    # 2. Catégoriel\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='constant', fill_value='Other')),\n",
    "        ('encoder', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1))\n",
    "    ])\n",
    "\n",
    "    # 3. Texte du Tweet (SVD 50 composants)\n",
    "    tweet_text_transformer = Pipeline(steps=[\n",
    "        ('tfidf', TfidfVectorizer(max_features=5000, stop_words='english', ngram_range=(1,2))),\n",
    "        ('svd', TruncatedSVD(n_components=50, random_state=42))\n",
    "    ])\n",
    "\n",
    "    # 4. Texte de la Bio (SVD 10 composants - c'est plus court)\n",
    "    # Cela permet de capter des thèmes (\"Politique\", \"Sport\") dans la bio\n",
    "    desc_text_transformer = Pipeline(steps=[\n",
    "        ('tfidf', TfidfVectorizer(max_features=1000, stop_words='english')),\n",
    "        ('svd', TruncatedSVD(n_components=10, random_state=42))\n",
    "    ])\n",
    "\n",
    "    # Assemblage global\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numeric_transformer, numeric_features),\n",
    "            ('cat', categorical_transformer, categorical_features),\n",
    "            ('tweet_txt', tweet_text_transformer, tweet_text_col),\n",
    "            ('desc_txt', desc_text_transformer, desc_text_col)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # --- Modèle ---\n",
    "    # On augmente légèrement la complexité (profondeur) car on a plus de données\n",
    "    model = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', HistGradientBoostingClassifier(\n",
    "            max_iter=300,        # Plus d'itérations\n",
    "            learning_rate=0.05,  # Apprentissage plus fin (plus lent mais plus précis)\n",
    "            max_depth=12,        # Arbres un peu plus profonds\n",
    "            max_leaf_nodes=40,\n",
    "            l2_regularization=1.0, # Évite le sur-apprentissage\n",
    "            random_state=42,\n",
    "            scoring='accuracy'\n",
    "        ))\n",
    "    ])\n",
    "\n",
    "    # --- Exécution ---\n",
    "    X = train_df\n",
    "    y = train_df['label']\n",
    "\n",
    "    print(\"-\" * 30)\n",
    "    print(\"Validation Croisée (5-Fold)...\")\n",
    "    kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    scores = cross_val_score(model, X, y, cv=kfold, scoring='accuracy', n_jobs=-1)\n",
    "    \n",
    "    print(f\"Scores CV : {scores}\")\n",
    "    print(f\"Moyenne CV : {np.mean(scores)*100:.2f}% (+/- {np.std(scores)*100:.2f}%)\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "    print(\"Entraînement Final...\")\n",
    "    model.fit(X, y)\n",
    "\n",
    "    print(\"Prédiction...\")\n",
    "    predictions = model.predict(test_df)\n",
    "\n",
    "    submission = pd.DataFrame({'ID': test_df['challenge_id'], 'Prediction': predictions})\n",
    "    submission.to_csv('submission_expert.csv', index=False)\n",
    "    print(\"Fichier 'submission_expert.csv' prêt.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chargement des données...\n",
      "Feature Engineering...\n",
      "--- Génération des features ---\n",
      "ATTENTION : Colonne 'screen_name' introuvable. Remplie avec des vides.\n",
      "--- Génération des features ---\n",
      "ATTENTION : Colonne 'screen_name' introuvable. Remplie avec des vides.\n",
      "------------------------------\n",
      "Validation Croisée (5-Fold)...\n",
      "Scores CV : [0.84097731 0.82574315 0.83875028 0.8331343  0.82983668]\n",
      "Moyenne CV : 83.37% (+/- 0.56%)\n",
      "------------------------------\n",
      "Entraînement Final...\n",
      "Prédiction...\n",
      "Fichier 'submission_expert.csv' prêt.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
