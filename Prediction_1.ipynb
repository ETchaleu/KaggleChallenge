{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 0. PACKAGE IMPORTS\n",
    "# ==========================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from pandas import json_normalize\n",
    "import re\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 1. FEATURE ENGINEERING\n",
    "# =========================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_source(source_html):\n",
    "    \"\"\"Extracts the source name from HTML anchor tag.\"\"\"\n",
    "    if pd.isna(source_html):\n",
    "        return \"Unknown\"\n",
    "    # Extract text between '>' and '<'\n",
    "    match = re.search(r'>(.*?)<', str(source_html))\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    return \"Unknown\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering(df):\n",
    "    print(\"--- Feature Generation ---\")\n",
    "    \n",
    "    # --- A. User Metadata ---\n",
    "    # Replace NaN with 0 for numerical user features\n",
    "    num_cols = ['user.followers_count', 'user.friends_count', 'user.listed_count', \n",
    "                'user.favourites_count', 'user.statuses_count']\n",
    "    for col in num_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].fillna(0)\n",
    "        else:\n",
    "            df[col] = 0\n",
    "\n",
    "    # 1. Advantage Ratio (Followers vs Friends)\n",
    "    # Log-transform to reduce skewness between large and small accounts\n",
    "    df['log_followers'] = np.log1p(df['user.followers_count'])\n",
    "    df['log_friends'] = np.log1p(df['user.friends_count'])\n",
    "    df['ratio_log'] = df['log_followers'] - df['log_friends'] # Equivalent to log(followers/friends)\n",
    "    \n",
    "    # 2. Liste Ratio\n",
    "    # 'listed_count' is often a sign of influence\n",
    "    df['user_listed_ratio'] = df['user.listed_count'] / (df['user.followers_count'] + 1)\n",
    "    \n",
    "    # --- B. Tweet Metadata (Behavior) ---\n",
    "    \n",
    "    # 3. Tweet Source (Professional Outlet vs Mobile User)\n",
    "    # Often : \"Influencers\" - TweetDeck, Buffer, Hootsuite.  \"Observers\" - Android/iPhone.\n",
    "    df['source_clean'] = df['source'].apply(extract_source)\n",
    "    # We can categorize sources into 'Top' and 'Other'\n",
    "    top_sources = ['Twitter for iPhone', 'Twitter for Android', 'Twitter Web App', \n",
    "                   'TweetDeck', 'iPad', 'Hootsuite', 'Buffer']\n",
    "    df['source_category'] = df['source_clean'].apply(lambda x: x if x in top_sources else 'Other')\n",
    "\n",
    "    # 4. Content Richness (Entities)\n",
    "    # An influencer often uses more hashtags, mentions, and links to engage audience\n",
    "    def count_entities(entity_list):\n",
    "        if isinstance(entity_list, list):\n",
    "            return len(entity_list)\n",
    "        return 0\n",
    "\n",
    "    # Count hashtags, URLs, and mentions\n",
    "    if 'entities.hashtags' in df.columns:\n",
    "        df['num_hashtags'] = df['entities.hashtags'].apply(count_entities)\n",
    "    else:\n",
    "        df['num_hashtags'] = 0\n",
    "        \n",
    "    if 'entities.urls' in df.columns:\n",
    "        df['num_urls'] = df['entities.urls'].apply(count_entities)\n",
    "    else:\n",
    "        df['num_urls'] = 0\n",
    "        \n",
    "    if 'entities.user_mentions' in df.columns:\n",
    "        df['num_mentions'] = df['entities.user_mentions'].apply(count_entities)\n",
    "    else:\n",
    "        df['num_mentions'] = 0\n",
    "\n",
    "    # 5. Virality Indicators\n",
    "    # Influential tweets often get more retweets and likes\n",
    "    df['retweet_count'] = df['retweet_count'].fillna(0)\n",
    "    df['favorite_count'] = df['favorite_count'].fillna(0)\n",
    "\n",
    "    # --- C. Text Cleaning ---\n",
    "    def get_text(row):\n",
    "        text = str(row.get('text', ''))\n",
    "        if 'extended_tweet.full_text' in row and not pd.isna(row['extended_tweet.full_text']):\n",
    "            text = str(row['extended_tweet.full_text'])\n",
    "        return text\n",
    "    \n",
    "    df['final_text'] = df.apply(get_text, axis=1)\n",
    "    df['text_len'] = df['final_text'].apply(len) # Tweet length as a feature\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 2. PIPELINE\n",
    "# =========================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run():\n",
    "    print(\"Data loading...\")\n",
    "    train_df = pd.read_json('train.jsonl', lines=True)\n",
    "    test_df = pd.read_json('kaggle_test.jsonl', lines=True)\n",
    "\n",
    "    print(\"JSON Flattening...\")\n",
    "    train_df = json_normalize(train_df.to_dict(orient='records'))\n",
    "    test_df = json_normalize(test_df.to_dict(orient='records'))\n",
    "\n",
    "    # Feature Engineering\n",
    "    train_df = feature_engineering(train_df)\n",
    "    test_df = feature_engineering(test_df)\n",
    "\n",
    "    # Columns for Modeling\n",
    "    numeric_features = [\n",
    "        'user.followers_count', 'user.friends_count', 'user.listed_count', \n",
    "        'user.favourites_count', 'user.statuses_count',\n",
    "        'ratio_log', 'user_listed_ratio',\n",
    "        'retweet_count', 'favorite_count',\n",
    "        'num_hashtags', 'num_urls', 'num_mentions', 'text_len'\n",
    "    ]\n",
    "\n",
    "    # Categorial Features\n",
    "    categorical_features = ['source_category']\n",
    "\n",
    "    # Text Feature\n",
    "    text_feature = 'final_text'\n",
    "\n",
    "    # --- Transformers Constructors ---\n",
    "\n",
    "    # 1. Imputation and Scaling for Numerical Features\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "\n",
    "    # 2. Ordinal Encoding for Categorical Features\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='constant', fill_value='Other')),\n",
    "        ('encoder', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1))\n",
    "    ])\n",
    "\n",
    "    # 3. Texte : TF-IDF + SVD (Dimension Reduction)\n",
    "    # HistGradientBoosting - no sparse input support, so we reduce dimensionality.\n",
    "    # We choose 50 components to balance information retention and efficiency.\n",
    "    text_transformer = Pipeline(steps=[\n",
    "        ('tfidf', TfidfVectorizer(max_features=5000, stop_words='english', ngram_range=(1,2))),\n",
    "        ('svd', TruncatedSVD(n_components=50, random_state=42)) \n",
    "    ])\n",
    "\n",
    "    # Assembling the Preprocessor\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numeric_transformer, numeric_features),\n",
    "            ('cat', categorical_transformer, categorical_features),\n",
    "            ('txt', text_transformer, text_feature)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Model : HistGradientBoostingClassifier\n",
    "    # State of the Art for tabular data in sklearn (similaire Ã  LightGBM)\n",
    "    model = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', HistGradientBoostingClassifier(\n",
    "            max_iter=200,           # Trees count\n",
    "            learning_rate=0.1,      # Learning rate\n",
    "            max_depth=10,           # Max depth of each tree\n",
    "            random_state=42,\n",
    "            scoring='accuracy'\n",
    "        ))\n",
    "    ])\n",
    "\n",
    "    # --- Training and Validation ---\n",
    "    X = train_df\n",
    "    y = train_df['label']\n",
    "\n",
    "    print(\"-\" * 30)\n",
    "    print(\"Cross-Validation (5-Fold)...\")\n",
    "    kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    scores = cross_val_score(model, X, y, cv=kfold, scoring='accuracy', n_jobs=-1)\n",
    "    \n",
    "    print(f\"CV Scores: {scores}\")\n",
    "    print(f\"CV Mean: {np.mean(scores)*100:.2f}% (+/- {np.std(scores)*100:.2f}%)\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "    # Final Training\n",
    "    print(\"Final Training...\")\n",
    "    model.fit(X, y)\n",
    "\n",
    "    # Prediction\n",
    "    print(\"Prediction on the test set...\")\n",
    "    predictions = model.predict(test_df)\n",
    "\n",
    "    # Save Submission\n",
    "    submission = pd.DataFrame({\n",
    "        'ID': test_df['challenge_id'],\n",
    "        'Prediction': predictions\n",
    "    })\n",
    "    submission.to_csv('Prediction.csv', index=False)\n",
    "    print(\"Finished! File 'Prediction.csv' is completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loading...\n",
      "JSON Flattening...\n",
      "--- Feature Generation ---\n",
      "--- Feature Generation ---\n",
      "------------------------------\n",
      "Cross-Validation (5-Fold)...\n",
      "CV Scores: [0.82632411 0.82419391 0.8258077  0.8247426  0.81966949]\n",
      "CV Mean: 82.41% (+/- 0.24%)\n",
      "------------------------------\n",
      "Final Training...\n",
      "Prediction on the test set...\n",
      "Finished! File 'Prediction.csv' is completed.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
